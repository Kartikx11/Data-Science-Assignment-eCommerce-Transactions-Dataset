import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import davies_bouldin_score
import matplotlib.pyplot as plt
import seaborn as sns

# Load the datasets
customers_path = "Customers.csv"
transactions_path = "Transactions.csv"

customers_df = pd.read_csv(customers_path)
transactions_df = pd.read_csv(transactions_path)

# Preprocessing
# Aggregate transaction data for each customer
customer_transactions = transactions_df.groupby("CustomerID").agg({
    "TotalValue": "sum",
    "Quantity": "sum"
}).reset_index()

# Merge aggregated data with customer profiles
customer_data = customers_df.merge(customer_transactions, on="CustomerID", how="left")

# Fill missing values with 0 (in case of customers with no transactions)
customer_data["TotalValue"] = customer_data["TotalValue"].fillna(0)
customer_data["Quantity"] = customer_data["Quantity"].fillna(0)

# One-hot encode the 'Region' column
customer_data = pd.get_dummies(customer_data, columns=["Region"], drop_first=True)

# Standardize the features
scaler = StandardScaler()
features = customer_data.drop(columns=["CustomerID", "CustomerName", "SignupDate"])
scaled_features = scaler.fit_transform(features)

# Determine the optimal number of clusters using the Davies-Bouldin Index
db_scores = []
cluster_range = range(2, 11)
for k in cluster_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(scaled_features)
    db_index = davies_bouldin_score(scaled_features, kmeans.labels_)
    db_scores.append(db_index)

# Plot the Davies-Bouldin Index for each number of clusters
plt.figure(figsize=(10, 6))
plt.plot(cluster_range, db_scores, marker='o')
plt.title('Davies-Bouldin Index for Different Numbers of Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('Davies-Bouldin Index')
plt.show()

# Choose the optimal number of clusters (e.g., where DB Index is lowest)
optimal_clusters = cluster_range[db_scores.index(min(db_scores))]
print(f"Optimal number of clusters: {optimal_clusters}")

# Perform KMeans clustering with the optimal number of clusters
kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)
customer_data["Cluster"] = kmeans.fit_predict(scaled_features)

# Save the cluster results
output_path = "Customer_Clusters.csv"
customer_data[["CustomerID", "Cluster"]].to_csv(output_path, index=False)
print(f"Clustering results saved to {output_path}.")

# Visualize the clusters
sns.pairplot(customer_data, hue="Cluster", diag_kind="kde", palette="tab10")
plt.show()